# OpenTelemetry Collector Configuration
# Simplified for your observability stack

# Define the protocols to receive data for.
receivers:
  # Configure receiving OTLP data via gRPC on port 4317 and HTTP on port 4318.
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318

  # Prometheus receiver to scrape metrics from your services
  prometheus:
    config:
      scrape_configs:
        # Scrape Loki metrics
        - job_name: 'loki'
          scrape_interval: 15s
          static_configs:
            - targets: ['loki:3100']
              labels:
                service: 'loki'
                group: 'infrastructure'

        # Scrape Tempo metrics
        - job_name: 'tempo'
          scrape_interval: 15s
          static_configs:
            - targets: ['tempo:3200']
              labels:
                service: 'tempo'
                group: 'infrastructure'

        # Scrape Grafana metrics
        - job_name: 'grafana'
          scrape_interval: 15s
          static_configs:
            - targets: ['grafana:3000']
              labels:
                service: 'grafana'
                group: 'infrastructure'

        # Scrape Prometheus metrics
        - job_name: 'prometheus'
          scrape_interval: 15s
          static_configs:
            - targets: ['prometheus:9090']
              labels:
                service: 'prometheus'
                group: 'infrastructure'

        # Scrape your FastAPI applications
        - job_name: 'fastapi-apps'
          scrape_interval: 5s
          static_configs:
            - targets: ['app-a:8000']
              labels:
                service: 'app-a'
                group: 'applications'
            - targets: ['app-b:8000']
              labels:
                service: 'app-b'
                group: 'applications'
            - targets: ['app-c:8000']
              labels:
                service: 'app-c'
                group: 'applications'

# Define processors to process received data.
processors:
  # Batch processor batches data before export (improves performance)
  batch:
    timeout: 10s
    send_batch_size: 1024

  # Memory limiter prevents OOM by dropping data when memory usage is high
  memory_limiter:
    check_interval: 1s
    limit_mib: 512
    spike_limit_mib: 128

  # Transform processor to rename span metrics (optional but recommended)
  transform:
    metric_statements:
      - context: metric
        statements:
          # Rename duration to latency for consistency
          - set(metric.name, "traces.spanmetrics.latency") where metric.name == "traces.spanmetrics.duration"
          # Add .total suffix to call counts
          - set(metric.name, "traces.spanmetrics.calls.total") where metric.name == "traces.spanmetrics.calls"

# Connectors generate metrics from traces
connectors:
  # Generate span metrics (RED metrics: Rate, Errors, Duration)
  spanmetrics:
    namespace: traces.spanmetrics
    metrics_flush_interval: 15s
    histogram:
      explicit:
        buckets: [0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2.5, 5, 10]
    dimensions:
      - name: http.method
      - name: http.target
      - name: http.status_code
      - name: service.version
    exemplars:
      enabled: true

  # Generate service graph metrics (service-to-service relationships)
  servicegraph:
    metrics_flush_interval: 15s
    dimensions:
      - http.method
      - http.status_code
    store:
      ttl: 2s
      max_items: 1000

## Define exporters to send data to backends
exporters:
  otlp/tempo:
    endpoint: tempo:4317  # send traces to Tempo
    tls:
      insecure: true

  # Export metrics to Prometheus
  prometheus:
    endpoint: "0.0.0.0:8889"
    namespace: otel
    send_timestamps: true
    metric_expiration: 5m

  # Debug exporter (useful for troubleshooting)
  debug:
    verbosity: basic
    sampling_initial: 5
    sampling_thereafter: 200

# Define the service pipelines
service:
  # Configure telemetry for the collector itself
  telemetry:
    logs:
      level: debug

  # Define processing pipelines
  pipelines:
    # Traces pipeline: receive -> process -> export to Tempo
    traces:
      receivers: [otlp]
      processors: [memory_limiter, batch]
      exporters: [otlp/tempo, spanmetrics, servicegraph]

    # Metrics pipeline: receive -> process -> export to Prometheus
    metrics:
      receivers: [otlp, prometheus, spanmetrics, servicegraph]
      processors: [memory_limiter, transform, batch]
      exporters: [prometheus]
